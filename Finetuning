{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unsloth # install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n!pip install torch transformers accelerate datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:17:19.261915Z","iopub.execute_input":"2025-02-03T05:17:19.262204Z","iopub.status.idle":"2025-02-03T05:18:03.907276Z","shell.execute_reply.started":"2025-02-03T05:17:19.262179Z","shell.execute_reply":"2025-02-03T05:18:03.906208Z"}},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2025.1.8-py3-none-any.whl.metadata (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2025.1.4 (from unsloth)\n  Downloading unsloth_zoo-2025.1.5-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.29.post2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes (from unsloth)\n  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.13-py3-none-any.whl.metadata (9.4 kB)\nCollecting transformers!=4.47.0,>=4.46.1 (from unsloth)\n  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.2.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.2.1)\nCollecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.27.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.1.4->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.1.4->unsloth) (11.0.0)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nINFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.12.14)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\nDownloading unsloth-2025.1.8-py3-none-any.whl (175 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.14.0-py3-none-any.whl (313 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.1.5-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.13-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nInstalling collected packages: triton, shtab, tyro, cut_cross_entropy, transformers, trl, xformers, unsloth_zoo, bitsandbytes, unsloth\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\nSuccessfully installed bitsandbytes-0.45.1 cut_cross_entropy-25.1.1 shtab-1.7.1 transformers-4.48.2 triton-3.1.0 trl-0.14.0 tyro-0.9.13 unsloth-2025.1.8 unsloth_zoo-2025.1.5 xformers-0.0.29.post1\nCollecting git+https://github.com/unslothai/unsloth.git\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-15l9w4qm\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-15l9w4qm\n  Resolved https://github.com/unslothai/unsloth.git to commit 038e6d4c8d40207a87297ab3aaf787c19b1006d1\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for unsloth: filename=unsloth-2025.1.8-py3-none-any.whl size=174982 sha256=c47fa0ae2ed015291fbc1a1ceb6e2e2eb78dec8fb2f5d85822637d09097e3d5b\n  Stored in directory: /tmp/pip-ephem-wheel-cache-oca2hjub/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\nSuccessfully built unsloth\nInstalling collected packages: unsloth\n  Attempting uninstall: unsloth\n    Found existing installation: unsloth 2025.1.8\n    Uninstalling unsloth-2025.1.8:\n      Successfully uninstalled unsloth-2025.1.8\nSuccessfully installed unsloth-2025.1.8\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.48.2)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Import all revelant packages","metadata":{}},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch                               # Pytorch\nfrom trl import SFTTrainer                 # Trainer for supervised finetuing (SFT)\nfrom unsloth import is_bfloat16_supported  # Checks if hardware supports bfloat16 precision\n# Hugging face modules\nfrom huggingface_hub import login          # Hugging face login\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset          # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import Kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:18:11.647947Z","iopub.execute_input":"2025-02-03T05:18:11.648251Z","iopub.status.idle":"2025-02-03T05:18:50.947638Z","shell.execute_reply.started":"2025-02-03T05:18:11.648225Z","shell.execute_reply":"2025-02-03T05:18:50.946774Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Create API keys and login to hugging face and weights and biases","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nhugging_face_token = user_secrets.get_secret(\"hf_token_read\")\nwnb_token = user_secrets.get_secret(\"wandb\")\n\nlogin(hugging_face_token) # Login to Hugging Face\nwandb.login(key=wnb_token) # Login to Weights and Biases\nrun = wandb.init(\n    project=\"Fine-tuning-v3\",\n    job_type=\"training\",\n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:20:19.997059Z","iopub.execute_input":"2025-02-03T05:20:19.997355Z","iopub.status.idle":"2025-02-03T05:20:32.741075Z","shell.execute_reply.started":"2025-02-03T05:20:19.997332Z","shell.execute_reply":"2025-02-03T05:20:32.740466Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabhi9ab\u001b[0m (\u001b[33mabhi9ab-rv-institute-of-technology-and-management\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250203_052026-buixvzcm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3/runs/buixvzcm' target=\"_blank\">swift-river-4</a></strong> to <a href='https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3' target=\"_blank\">https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3/runs/buixvzcm' target=\"_blank\">https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3/runs/buixvzcm</a>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Loading Deepseek R1 and Tokenizer","metadata":{}},{"cell_type":"code","source":"# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e how many tokens can be processed at once)\ndtype = None          # Set to default datatype\nload_in_4bit = True   # Enable 4-bit quantization - a memory saving optimization\n\n# Load Deepseek R1 model and tokenizer using unsloth\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = hugging_face_token\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:20:48.442600Z","iopub.execute_input":"2025-02-03T05:20:48.442895Z","iopub.status.idle":"2025-02-03T05:21:11.297050Z","shell.execute_reply.started":"2025-02-03T05:20:48.442872Z","shell.execute_reply":"2025-02-03T05:21:11.296392Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.1.8: Fast Llama patching. Transformers: 4.48.2.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec4fcd0a54e4599a5ef72ea9eac3407"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c562750b83ab41b5a1f9b78deddfaffb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e8ef49c5c247f08f1dea76f16b896e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59959e1cac0f42189ff4e75014e26c3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bf016681df3423489ed035549ff69d0"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Testing Deepseek R1 on a finance use-case before tuning\n\n### Define a system prompt","metadata":{}},{"cell_type":"code","source":"prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a GenAI-based financial assistant designed to help users with basic and advanced questions about investing. Your goal is to provide clear, accurate, and actionable information to improve financial literacy and guide users towards informed investment decisions. You should also help users discover suitable investment products based on their financial needs and goals.\nPlease answer the following finance-related question.\n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:21:16.915281Z","iopub.execute_input":"2025-02-03T05:21:16.915786Z","iopub.status.idle":"2025-02-03T05:21:16.922103Z","shell.execute_reply.started":"2025-02-03T05:21:16.915744Z","shell.execute_reply":"2025-02-03T05:21:16.921302Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Creating a question\nquestion = \"\"\"I’m 35 years old, earn ₹1.5 lakh per month, and have ₹10 lakh in savings. \n              I want to invest for my child’s education (expected cost ₹50 lakh in 10 years) and my retirement (planned at age 60).\n              I’m willing to take moderate risk. Can you suggest a diversified investment plan to achieve these goals?\"\"\"\n\n# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\nFastLanguageModel.for_inference(model)\n\n# Format the question using the structured prompt and tokenize it\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors = \"pt\").to(\"cuda\")\n\n# Generate a response using the model\noutputs = model.generate(\n    input_ids = inputs.input_ids,\n    attention_mask = inputs.attention_mask,\n    max_new_tokens = 1200,\n    use_cache = True\n)\n\n# Decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:21:19.475333Z","iopub.execute_input":"2025-02-03T05:21:19.475667Z","iopub.status.idle":"2025-02-03T05:22:28.460702Z","shell.execute_reply.started":"2025-02-03T05:21:19.475639Z","shell.execute_reply":"2025-02-03T05:22:28.460013Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Extract and print only the relevant response part (after \"### Response:\")\nprint(response[0].split(\"### Response\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:22:31.525526Z","iopub.execute_input":"2025-02-03T05:22:31.525881Z","iopub.status.idle":"2025-02-03T05:22:31.532518Z","shell.execute_reply.started":"2025-02-03T05:22:31.525854Z","shell.execute_reply":"2025-02-03T05:22:31.531722Z"}},"outputs":[{"name":"stdout","text":":\n<think>\nOkay, so I'm trying to help this user create a diversified investment plan. They're 35, earn ₹1.5 lakh a month, and have ₹10 lakh in savings. Their goals are to fund their child's education, which is expected to cost ₹50 lakh in 10 years, and also plan for retirement at age 60. They're willing to take moderate risk.\n\nFirst, I need to figure out how much they need to save for each goal. For the child's education, the target is ₹50 lakh in 10 years. Assuming they don't contribute anything else, they need to save about ₹5,000 a month, which seems doable given their salary. But since they have ₹10 lakh, maybe they can contribute more or use some of it for the education fund.\n\nFor retirement, at age 60, they'll need a corpus of around ₹3.5 lakh per year, considering inflation and a comfortable lifestyle. To build this, over 25 years, they need to save roughly ₹14,000 a month, which is a significant amount. Given their current savings, they might need to adjust their contributions or investments to meet this.\n\nSince they're willing to take moderate risk, a balanced portfolio with equities, debt, and maybe some alternative assets could work. They should also consider tax efficiency, so using tax-saving instruments like EPF or PPF might be beneficial.\n\nI should suggest a mix of equities (maybe 50% in index funds or ETFs for growth), debt (like low-risk bonds or fixed deposits for stability), and maybe a small portion in alternative assets like gold or real estate for diversification. They should also consider their emergency fund, ensuring they have enough liquidity.\n\nTax planning is important too. They can use tools like tax-saving mutual funds or invest in tax-efficient equity instruments. Also, utilizing their employer's benefits like EPF can help.\n\nI should also advise them to review their portfolio annually and adjust based on their goals and market conditions. Maybe include some steps on how to allocate their funds and the reasoning behind each choice.\n\nFinally, ensure the plan is actionable and tailored to their specific situation, considering their income, savings, and risk tolerance.\n</think>\n\n**Investment Plan for the User**\n\n**Objective:**  \nTo achieve the goals of funding the child's education and ensuring a comfortable retirement, while maintaining a moderate risk profile.\n\n---\n\n### **1. Education Fund (₹50 lakh in 10 years)**\n- **Current Savings:** ₹10 lakh\n- **Additional Contribution Needed:** ₹40 lakh\n- **Monthly Contribution:** ₹5,000 (from current savings) + potential increases\n\n**Investment Strategy:**\n- **Equity Investments (50% allocation):** \n  - Focus on low-risk equities or index funds (e.g., Nifty 50) for long-term growth.\n  - Example: Invest ₹25,000 in Nifty 50 ETF.\n- **Debt Investments (30% allocation):**\n  - Opt for low-risk bonds or fixed deposits for stability.\n  - Example: Invest ₹15,000 in a low-risk bond or FD.\n- **Alternative Assets (20% allocation):**\n  - Consider gold or real estate for diversification.\n  - Example: Invest ₹10,000 in gold ETFs or a portion in REITs.\n\n**Emergency Fund:**\n- Maintain at least 3-6 months of expenses in liquid assets (e.g., savings account).\n\n---\n\n### **2. Retirement Fund (Targeting ₹3.5 lakh/year by 60)**\n- **Timeframe:** 25 years\n- **Required Annual Corpus:** ₹3.5 lakh\n- **Total Required Sum:** ₹87.5 lakh (assuming 3% annual withdrawal)\n\n**Investment Strategy:**\n- **Equity Investments (60% allocation):**\n  - Focus on moderate-risk equities for growth.\n  - Example: Invest ₹54,000 in a diversified equity portfolio.\n- **Debt Investments (30% allocation):**\n  - Use for stability and to hedge against inflation.\n  - Example: Invest ₹18,000 in fixed deposits or low-risk bonds.\n- **Alternative Assets (10% allocation):**\n  - Include gold or other alternative assets.\n  - Example: Invest ₹9,000 in gold ETFs.\n\n**Tax Planning:**\n- Utilize tax-saving instruments like EPF and PPF.\n- Consider tax-efficient mutual funds and ETFs.\n\n---\n\n### **3. Additional Considerations**\n- **Employer Benefits:** Maximize EPF contributions for tax benefits.\n- **Retirement Accounts:** Open a PPF or tax-saving mutual fund.\n- **Review Portfolio Annually:** Adjust based on market conditions and goals.\n\n---\n\n### **Conclusion**\nThis plan balances growth and stability, aligns with moderate risk, and is tailored to your financial situation. Regularly review and adjust your investments to ensure progress toward both your child's education and retirement goals.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Fine-tuning step by step\n\n### Step 1 - Update the system prompt","metadata":{}},{"cell_type":"code","source":"# Update training prompt style to add </think> tag\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a GenAI-based financial assistant designed to help users with basic and advanced questions about investing. Your goal is to provide clear, accurate, and actionable information to improve financial literacy and guide users towards informed investment decisions. You should also help users discover suitable investment products based on their financial needs and goals.\nPlease answer the following finance-related question.\n\n### Question:\n{}\n\n### Response:\n<think>\nLet me break this down step by step:\n1. Understand the key concepts being asked about\n2. Identify the main differences or important points\n3. Structure a clear and concise response\n</think>\n{}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:22:38.172851Z","iopub.execute_input":"2025-02-03T05:22:38.173125Z","iopub.status.idle":"2025-02-03T05:22:38.177589Z","shell.execute_reply.started":"2025-02-03T05:22:38.173105Z","shell.execute_reply":"2025-02-03T05:22:38.176903Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Step 2 - Download the fine-tuning dataset and format it for fune-tuning\n\nWe will be using the Finance-Instruct-500k found on [here](https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k).\n\nExample entry:\n```bash\n{\n  \"system\": \"You are a financial assistant. Answer the user's question accurately but keep it brief.\",\n  \"user\": \"What is the difference between stocks and bonds?\",\n  \"assistant\": \"Stocks represent ownership in a company, while bonds are a form of debt where the investor lends money to an entity.\"\n}\n```","metadata":{}},{"cell_type":"code","source":"# Download the dataset using hugging face\ndataset = load_dataset(\"Josephgflowers/Finance-Instruct-500k\", split = \"train[0:5000]\", trust_remote_code = True)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:22:43.760241Z","iopub.execute_input":"2025-02-03T05:22:43.760524Z","iopub.status.idle":"2025-02-03T05:22:49.592792Z","shell.execute_reply.started":"2025-02-03T05:22:43.760502Z","shell.execute_reply":"2025-02-03T05:22:49.592037Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99c082bbc5714b3d98c1fd3e6f69e249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.json:   0%|          | 0.00/580M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"398399a4e87e49828a63dc7476dcb255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/518185 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3803fbc17da145edb3805107420da149"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['system', 'user', 'assistant'],\n    num_rows: 5000\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Show an entry from the dataset\ndataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:22:53.120308Z","iopub.execute_input":"2025-02-03T05:22:53.120608Z","iopub.status.idle":"2025-02-03T05:22:53.126759Z","shell.execute_reply.started":"2025-02-03T05:22:53.120583Z","shell.execute_reply":"2025-02-03T05:22:53.126016Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'system': '\\n',\n 'user': 'Explain the classical economic theory and its policy implications.',\n 'assistant': 'The classical economic theory refers to the economic thought of the 18th and 19th centuries associated with thinkers like Adam Smith, David Ricardo, and Thomas Malthus. Some of the key ideas of this school of thought are:\\n\\n• Laissez-faire approach: Classical economists believed in minimal government intervention in the economy. They argued that the free market, left to its own devices, would lead to the most efficient allocation of resources through the mechanism of supply and demand.\\n\\n• Self-interest and \"invisible hand\": Adam Smith proposed that when individuals pursue their own self-interest, they are led \"as if by an invisible hand\" to promote the general good of society. This provides a justification for the laissez-faire approach.\\n\\n• Wage-fund doctrine: Classical economists believed that wages were determined by a \"wage fund\" consisting of all capital available to hire labor. This suggested that government should not intervene to raise wages above the market rate.\\n\\n• Diminishing returns: Classical economists argued that as more labor and capital are applied to land, the additional output obtained diminishes, leading to an optimal level of inputs.\\n\\nThe main policy implications of classical economic theory are:\\n\\n• Minimal government intervention: Based on their belief in self-regulating markets and the invisible hand, classical economists argued for limited government regulation and spending, and against government price controls and wage controls.\\n\\n• Free trade: Classical economists supported free trade policies and opposed tariffs based on the idea of comparative advantage and specialization. They believed free trade maximizes overall economic output.\\n\\n• Gold standard: Many classical economists supported tying currencies to gold to ensure monetary stability and control inflation.\\n\\n• Non-distortion of market prices: Classical economists opposed most government policies that distort market prices and rates of return, which they viewed as interfering with the efficient allocation of resources.\\n\\nSo in summary, the classical economic theory favors free markets, limited government, free trade, and monetary stability - which translates into promoting laissez-faire economic policies.'}"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# We need to format the dataset to fit our prompt training style\nEOS_TOKEN = tokenizer.eos_token # Prevent the model from continuing beyond the expected response length by adding the EOS_TOKEN.\nEOS_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:22:56.900032Z","iopub.execute_input":"2025-02-03T05:22:56.900315Z","iopub.status.idle":"2025-02-03T05:22:56.906346Z","shell.execute_reply.started":"2025-02-03T05:22:56.900293Z","shell.execute_reply":"2025-02-03T05:22:56.905699Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<｜end▁of▁sentence｜>'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Define the formatting prompt function\ndef formatting_prompt_func(examples):\n    inputs = examples[\"user\"]\n    outputs = examples[\"assistant\"]\n\n    texts = []\n\n    for input, output in zip(inputs, outputs):\n        text = train_prompt_style.format(input, output) + EOS_TOKEN\n        texts.append(text)\n\n    return {\n        \"text\": texts\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:23:00.070616Z","iopub.execute_input":"2025-02-03T05:23:00.070978Z","iopub.status.idle":"2025-02-03T05:23:00.076287Z","shell.execute_reply.started":"2025-02-03T05:23:00.070955Z","shell.execute_reply":"2025-02-03T05:23:00.075632Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Update dataset formatting\ndataset_finetune = dataset.map(formatting_prompt_func, batched=True)\ndataset_finetune[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:23:04.415466Z","iopub.execute_input":"2025-02-03T05:23:04.415815Z","iopub.status.idle":"2025-02-03T05:23:04.609193Z","shell.execute_reply.started":"2025-02-03T05:23:04.415788Z","shell.execute_reply":"2025-02-03T05:23:04.608330Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9c22b6f9784aa5a5b34756c0050d1d"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"Below is an instruction that describes a task, paired with an input that provides further context.\\nWrite a response that appropriately completes the request.\\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a GenAI-based financial assistant designed to help users with basic and advanced questions about investing. Your goal is to provide clear, accurate, and actionable information to improve financial literacy and guide users towards informed investment decisions. You should also help users discover suitable investment products based on their financial needs and goals.\\nPlease answer the following finance-related question.\\n\\n### Question:\\nExplain tradeoffs between fiscal and monetary policy as tools in a nation's economic toolkit. Provide examples of past instances when each were utilized, the economic conditions that led to them being deployed, their intended effects, and an evaluation of their relative efficacy and consequences.\\n\\n### Response:\\n<think>\\nLet me break this down step by step:\\n1. Understand the key concepts being asked about\\n2. Identify the main differences or important points\\n3. Structure a clear and concise response\\n</think>\\nFiscal and monetary policy are the two main tools that governments have to influence economic activity. They each have benefits and drawbacks.\\n\\nFiscal policy refers to government spending and taxation decisions. Examples of fiscal policy include:\\n\\n• During the Great Recession, the U.S. government implemented a fiscal stimulus through the American Recovery and Reinvestment Act of 2009. This included increased spending on infrastructure, tax cuts, and expanded unemployment benefits. The intention was to boost aggregate demand and stimulate economic activity. Studies have found that the stimulus had a positive but modest impact on economic growth.\\n\\n• The Trump administration implemented tax cuts in 2017 with the goal of increasing business investment and growth. However, the effect on growth has been debated, and the tax cuts significantly increased the federal budget deficit.\\n\\nMonetary policy refers to decisions by a central bank, like the Federal Reserve, about interest rates and the money supply. Examples of monetary policy include:\\n\\n• After the 2008 financial crisis, the Fed lowered interest rates to near zero and implemented quantitative easing programs to increase liquidity. The intention was to boost lending, investment, and consumer spending. These actions are credited with helping the economy recover.\\n\\n• In the late 1990s, the Fed raised interest rates to reduce inflationary pressures from economic growth. Higher rates contributed to slowing the economy and avoiding a spike in prices.\\n\\nThe key tradeoffs are:\\n\\n• Fiscal policy can have a more targeted impact but takes longer to implement due to the political process. Monetary policy is more nimble but has a broader influence on the economy.\\n\\n• Fiscal policy impacts the budget deficit, while monetary policy does not directly affect government debt. However, very low interest rates due to monetary policy can encourage higher government borrowing. \\n\\n• Monetary policy tends to be less politically contentious, while fiscal policy proposals often face more opposition.\\n\\nIn summary, both tools have pros and cons and work best in tandem to stabilize the economy and maximize growth. The optimal mix depends on the specific economic conditions and policy goals.\\n<｜end▁of▁sentence｜>\""},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"### Step 3 - Setting up the model using LoRA\n\nAn intuitive explanation of LoRA\n\nLarge Language Models (LLMs) have **millions or even billions of weights** that determine how they process and generate text. While fine-tuning a model, we usually update all these weights, which requires **massive computational resources and memory.**\n\nLoRA (**Low-Rank Adaptation**) allows to fine-tune efficiently by:\n\n- Instead of modifying all weights, LoRA adds small, trainable adapters to specific layers.\n- These adapters capture task-specific knowledge while leaving the original model unchanged.\n- This reduces the number of trainable parameters by more than **90%**, making fine-tuning faster and more **memory-efficient**.","metadata":{}},{"cell_type":"code","source":"# Apply LoRA (Low-Rank Adaptation) fine-tuning to the model\nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  \n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,                         # Higher the number, more weight changes the lora process will do to the layers.\n    lora_dropout=0,                        # No Dropout. Dropout means how much information you retain in the weight updating process. Here, we are setting full retaintion of information.\n    bias=\"none\",                           # Whether the lora layers should learn bias terms. Essentially, a memory saving technique.\n    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context. Saves memory by recomputing the activations that we're doing instead of storing them.  \n    random_state=3407,                     # For preset ability. This is a random seed.\n    use_rslora=False,                      # A Technique of lora called rank stabilized lora.\n    loftq_config=None,                     # We disable this because we already have 4-bit quantization.\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:23:10.155623Z","iopub.execute_input":"2025-02-03T05:23:10.155924Z","iopub.status.idle":"2025-02-03T05:23:16.087685Z","shell.execute_reply.started":"2025-02-03T05:23:10.155901Z","shell.execute_reply":"2025-02-03T05:23:16.087008Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.1.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Initialize the fine-tuning trainer\ntrainer = SFTTrainer(\n    model=model_lora,\n    tokenizer=tokenizer,\n    train_dataset=dataset_finetune,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length, # 2048\n    dataset_num_proc=2, # Number of processors\n    args=TrainingArguments(\n        per_device_train_batch_size=2,     # Number of examples processed per device CPU at a time.\n        gradient_accumulation_steps=4,     # How many steps the gradient needs to accumulate before updating weights.\n        num_train_epochs = 1,              # warmup_ratio for full training runs!\n        warmup_steps=5,                    # Gradually increase the learning rate for the first five steps.\n        # max_steps=60,                      # How many steps are in the training process?\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),  # Float point 16. Use fp16 if bf16 not supported.\n        bf16=is_bfloat16_supported(),      # In case we have bf16 support.\n        logging_steps=10,                  # logs training process every 10 steps.\n        optim=\"adamw_8bit\",                # optimizer\n        weight_decay=0.01,                 # Allows regularization to prevent overfitting. Prevents the model to overfit on the dataset and become only accurate on training tasks.\n        lr_scheduler_type=\"linear\",        # Type of learning rate.\n        seed=3407,                         # Random seed for reproducibility.\n        output_dir=\"outputs\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:23:18.075805Z","iopub.execute_input":"2025-02-03T05:23:18.076089Z","iopub.status.idle":"2025-02-03T05:23:23.852885Z","shell.execute_reply.started":"2025-02-03T05:23:18.076068Z","shell.execute_reply":"2025-02-03T05:23:23.852196Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90b42f2388954057ac60045cf296ffb4"}},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"### Step 4 - Model training  ","metadata":{}},{"cell_type":"code","source":"# Start the fine-tuning process\ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T05:23:28.345228Z","iopub.execute_input":"2025-02-03T05:23:28.345569Z","iopub.status.idle":"2025-02-03T08:28:48.418076Z","shell.execute_reply.started":"2025-02-03T05:23:28.345522Z","shell.execute_reply":"2025-02-03T08:28:48.417354Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 5,000 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 625\n \"-____-\"     Number of trainable parameters = 41,943,040\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [625/625 3:04:48, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.994600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.145800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.059100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.995000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.980500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.945800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.922800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.954500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.941500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.995100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.931900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.941700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.937100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.908800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.915100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.932800</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.901700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.934000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.933400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.909200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.965500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.960100</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.939800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.887800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.931000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.895000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.948200</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.941900</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.905000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.897300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.892800</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.891900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.914700</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.893300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.953400</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.937900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.898500</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.914100</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.875400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.939500</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.901500</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.859800</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.869300</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.858200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.882600</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.872900</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.870200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.851400</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.884800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.918200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.885600</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.863600</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.886200</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.858300</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.905700</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.896400</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.902100</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.863100</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.826600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.849200</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.842800</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.913300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Save the fine-tuned model\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:29:08.726944Z","iopub.execute_input":"2025-02-03T08:29:08.727244Z","iopub.status.idle":"2025-02-03T08:29:10.121406Z","shell.execute_reply.started":"2025-02-03T08:29:08.727219Z","shell.execute_reply":"2025-02-03T08:29:10.120621Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▆▄▂▅▃▃▁▂▁▂▃▄▄▃▄▅▃▃▃▂▃▁▃▃▂▅▅▄▅▃▅▂▃▄▅▅▄▇█▃</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▅▄▄▄▄▃▄▃▄▄▃▅▄▃▄▃▅▃▃▃▃▄▅▄▄▂▄▃▂▂▃▂▃▃▂▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.3280189906588467e+17</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>625</td></tr><tr><td>train/grad_norm</td><td>0.31964</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.9133</td></tr><tr><td>train_loss</td><td>0.93193</td></tr><tr><td>train_runtime</td><td>11116.4359</td></tr><tr><td>train_samples_per_second</td><td>0.45</td></tr><tr><td>train_steps_per_second</td><td>0.056</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">swift-river-4</strong> at: <a href='https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3/runs/buixvzcm' target=\"_blank\">https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3/runs/buixvzcm</a><br> View project at: <a href='https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3' target=\"_blank\">https://wandb.ai/abhi9ab-rv-institute-of-technology-and-management/Fine-tuning-v3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250203_052026-buixvzcm/logs</code>"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"### Step 5 - Run model inference after fine-tuning","metadata":{}},{"cell_type":"code","source":"question = \"I’m 35 years old, earn ₹1.5 lakh per month, and have ₹10 lakh in savings. I want to invest for my child’s education (expected cost ₹50 lakh in 10 years) and my retirement (planned at age 60). I’m willing to take moderate risk. Can you suggest a diversified investment plan to achieve these goals?\"\n\n# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\nFastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,           # Tokenize input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200,                  # Maximum length for generating response\n    use_cache=True,                       # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after '### Response:'\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:29:26.772989Z","iopub.execute_input":"2025-02-03T08:29:26.773307Z","iopub.status.idle":"2025-02-03T08:29:56.824503Z","shell.execute_reply.started":"2025-02-03T08:29:26.773280Z","shell.execute_reply":"2025-02-03T08:29:56.823633Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nLet me break this down step by step:\n1. Understand the key concepts being asked about\n2. Identify the main differences or important points\n3. Structure a clear and concise response\n</think>\n1. Fixed deposits (FDs): Start with 2-3 years FDs for liquidity. This will give you 6-7% interest for the first 1-2 years. Then invest 10-15% of your total savings in FDs for 3-5 years to get 6-7% interest. \n\n2. Public Provident Fund (PPF): Invest 10-15% of your total savings in PPF for 15 years. This will give you 8-9% tax-free interest. PPF is a safe and tax-efficient option.\n\n3. Gold: Invest 10-15% of your total savings in gold. Gold is a hedge against inflation and currency fluctuations. However, gold prices are volatile so you need to manage this wisely.\n\n4. Mutual funds: Invest 30-40% of your total savings in actively managed equity funds and index funds for moderate risk. This will give you 8-10% average returns over 10-15 years. \n\n5. Real estate: Consider investing 10-15% of your savings in real estate for rental income. However, real estate is illiquid and comes with risks.\n\n6. Life insurance: Buy a term life insurance policy for 20-30 years to cover any unforeseen medical emergencies and provide a lump sum payout on death. This will give you 4-6% returns.\n\n7. Fixed deposit for child's education: Invest 10-15% of your total savings in a 15-year FD for the child's education. This will give you 6-7% interest.\n\n8. Retire at 60: Start a fixed deposit for 15-20 years to cover for 10-15 years of income after retirement. This will give you 6-7% interest.\n\nDoes this plan seem reasonable? Let me know if you would like me to modify or expand on any part of the recommendation.\n<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Saving the model locally","metadata":{}},{"cell_type":"code","source":"new_model_local = \"DeepSeek-R1-Distill-Llama-8B-finance-v1\"\nmodel_lora.save_pretrained(new_model_local) \ntokenizer.save_pretrained(new_model_local)\n\nmodel_lora.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:30:12.913068Z","iopub.execute_input":"2025-02-03T08:30:12.913372Z","iopub.status.idle":"2025-02-03T08:31:59.934668Z","shell.execute_reply.started":"2025-02-03T08:30:12.913347Z","shell.execute_reply":"2025-02-03T08:31:59.933937Z"}},"outputs":[{"name":"stderr","text":"Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\nWe shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\nTo force `safe_serialization`, set it to `None` instead.\nUnsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\nmodel which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\nUnsloth: Will remove a cached repo with size 6.0G\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 18.79 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 11/32 [00:00<00:01, 13.60it/s]\nWe will save to Disk and not RAM now.\n100%|██████████| 32/32 [00:24<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\nUnsloth: Saving DeepSeek-R1-Distill-Llama-8B-finance-v1/pytorch_model-00001-of-00004.bin...\nUnsloth: Saving DeepSeek-R1-Distill-Llama-8B-finance-v1/pytorch_model-00002-of-00004.bin...\nUnsloth: Saving DeepSeek-R1-Distill-Llama-8B-finance-v1/pytorch_model-00003-of-00004.bin...\nUnsloth: Saving DeepSeek-R1-Distill-Llama-8B-finance-v1/pytorch_model-00004-of-00004.bin...\nDone.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Pushing the model to Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"hugging_face_write_token = user_secrets.get_secret(\"hf_token_write\")\nlogin(hugging_face_write_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:33:26.162737Z","iopub.execute_input":"2025-02-03T08:33:26.163074Z","iopub.status.idle":"2025-02-03T08:33:26.419474Z","shell.execute_reply.started":"2025-02-03T08:33:26.163050Z","shell.execute_reply":"2025-02-03T08:33:26.418636Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"new_model_online = \"abhi9ab/DeepSeek-R1-Distill-Llama-8B-finance-v1\"\nmodel_lora.push_to_hub(new_model_online)\ntokenizer.push_to_hub(new_model_online)\n\nmodel_lora.push_to_hub_merged(new_model_online, tokenizer, save_method = \"merged_16bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:33:32.891406Z","iopub.execute_input":"2025-02-03T08:33:32.891747Z","iopub.status.idle":"2025-02-03T08:38:24.794108Z","shell.execute_reply.started":"2025-02-03T08:33:32.891720Z","shell.execute_reply":"2025-02-03T08:38:24.793152Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/626 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa985b2999f4b4d969af8f77ee6df6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b4982ebc0c4efda6a596a4f3cae78f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3f0e01eeee44d80a6a13dd9809221e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a4a8371b07447b28f5f92f2b9e77ec4"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/abhi9ab/DeepSeek-R1-Distill-Llama-8B-finance-v1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ebf827d08bc428c98e656afeee5b160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af9df5817b99460eae621b9926c335ec"}},"metadata":{}},{"name":"stderr","text":"Unsloth: You are pushing to hub in Kaggle environment.\nTo save memory, we shall move abhi9ab/DeepSeek-R1-Distill-Llama-8B-finance-v1 to /tmp/DeepSeek-R1-Distill-Llama-8B-finance-v1\nUnsloth: Will remove a cached repo with size 1.6K\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 18.84 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 32/32 [00:23<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer...","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":" Done.\nUnsloth: Saving /tmp/DeepSeek-R1-Distill-Llama-8B-finance-v1/pytorch_model-00001-of-00004.bin...\nUnsloth: Saving /tmp/DeepSeek-R1-Distill-Llama-8B-finance-v1/pytorch_model-00002-of-00004.bin...\nUnsloth: Saving /tmp/DeepSeek-R1-Distill-Llama-8B-finance-v1/pytorch_model-00003-of-00004.bin...\nUnsloth: Saving /tmp/DeepSeek-R1-Distill-Llama-8B-finance-v1/pytorch_model-00004-of-00004.bin...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d3c3d1d4234825a1440fe21c0b40b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00004.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4ea3d1aee4943a79e7e652b6efff8e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00004.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40edfc0fa5744a6b8818ec65c5b1919e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00004.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f829e96eb764cbe8e92a430e73ef0a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00004.bin:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"782a7a65fbfd4d88a88ec63ea7eef086"}},"metadata":{}},{"name":"stdout","text":"Done.\nSaved merged model to https://huggingface.co/abhi9ab/DeepSeek-R1-Distill-Llama-8B-finance-v1\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}